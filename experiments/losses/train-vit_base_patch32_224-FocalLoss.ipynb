{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Jupyter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "from sklearn.metrics import f1_score, accuracy_score, top_k_accuracy_score\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pd.set_option('display.max_columns', None)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fgvc.utils.datasets import TrainDataset\n",
    "from fgvc.utils.augmentations import light_transforms, heavy_transforms\n",
    "from fgvc.utils.utils import timer, init_logger, seed_everything, getModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May 19 11:57:26 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.27       Driver Version: 465.27       CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| 30%   26C    P8    24W / 350W |   2630MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:C1:00.0 Off |                  N/A |\n",
      "| 30%   26C    P8    26W / 350W |      3MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     51194      C   /opt/conda/bin/python            2627MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286841\n",
      "33703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (15,16,17,19,22) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "train_metadata = pd.read_csv(\"../../metadata/PlantCLEF2018_train_metadata.csv\")\n",
    "print(len(train_metadata))\n",
    "\n",
    "val_metadata = pd.read_csv(\"../../metadata/PlantCLEF2018_val_metadata.csv\")\n",
    "print(len(val_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata['image_path'] = train_metadata['image_path'].apply(lambda x: x.replace('../../../nahouby/Datasets/PlantCLEF/', '/local/nahouby/Datasets/PlantCLEF/'))\n",
    "train_metadata['image_path'] = train_metadata['image_path'].apply(lambda x: x.replace('../../nahouby/Datasets/PlantCLEF/', '/local/nahouby/Datasets/PlantCLEF/'))\n",
    "\n",
    "val_metadata['image_path'] = val_metadata['image_path'].apply(lambda x: x.replace('../../../nahouby/Datasets/PlantCLEF/', '/local/nahouby/Datasets/PlantCLEF/'))\n",
    "val_metadata['image_path'] = val_metadata['image_path'].apply(lambda x: x.replace('../../nahouby/Datasets/PlantCLEF/', '/local/nahouby/Datasets/PlantCLEF/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust BATCH_SIZE and ACCUMULATION_STEPS to values that if multiplied results in 64 !!!!!1\n",
    "\n",
    "config = {\"augmentations\": 'light',\n",
    "           \"optimizer\": 'SGD',\n",
    "           \"scheduler\": 'plateau',\n",
    "           \"image_size\": (224, 224),\n",
    "           \"random_seed\": 777,\n",
    "           \"number_of_classes\": len(train_metadata['class_id'].unique()),\n",
    "           \"architecture\": 'vit_base_patch32_224',\n",
    "           \"batch_size\": 32,\n",
    "           \"accumulation_steps\": 4,\n",
    "           \"epochs\": 100,\n",
    "           \"learning_rate\": 0.01,\n",
    "           \"dataset\": 'PlantCLEF2018',\n",
    "           \"loss\": 'FocalLoss',\n",
    "           \"training_samples\": len(train_metadata),\n",
    "           \"valid_samples\": len(val_metadata),\n",
    "           \"workers\": 12}\n",
    "\n",
    "RUN_NAME = f\"{config['architecture']}-{config['optimizer']}-{config['scheduler']}-{config['augmentations']}-{config['loss']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Seeds & Log Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FILE = f'{RUN_NAME}.log'\n",
    "LOGGER = init_logger(LOG_FILE)\n",
    "\n",
    "seed_everything(config['random_seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "model = getModel(config['architecture'], config['number_of_classes'], pretrained=True)\n",
    "model_mean = list(model.default_cfg['mean'])\n",
    "model_std = list(model.default_cfg['std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentations: light\n"
     ]
    }
   ],
   "source": [
    "# Adjust BATCH_SIZE and ACCUMULATION_STEPS to values that if multiplied results in 64 !!!!!1\n",
    "\n",
    "if config['augmentations'] == 'light':\n",
    "    train_augmentations = light_transforms(data='train', image_size=config['image_size'], mean=model_mean, std=model_std)\n",
    "    val_augmentations = light_transforms(data='valid', image_size=config['image_size'], mean=model_mean, std=model_std)\n",
    "elif config['augmentations'] == 'light-random_crop':\n",
    "    train_augmentations = light_transforms_rcrop(data='train', image_size=config['image_size'], mean=model_mean, std=model_std)\n",
    "    val_augmentations = light_transforms_rcrop(data='valid-center-crop', image_size=config['image_size'], mean=model_mean, std=model_std)    \n",
    "elif config['augmentations'] == 'heavy_transforms':\n",
    "    train_augmentations = heavy_transforms(data='train', image_size=config['image_size'], mean=model_mean, std=model_std)\n",
    "    val_augmentations = heavy_transforms(data='valid', image_size=config['image_size'], mean=model_mean, std=model_std)    \n",
    "\n",
    "    \n",
    "    \n",
    "print('Augmentations:', config['augmentations'])\n",
    "\n",
    "train_dataset = TrainDataset(train_metadata, transform=train_augmentations)\n",
    "valid_dataset = TrainDataset(val_metadata, transform=val_augmentations)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=config['workers'])\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=config['workers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicekl\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/local/picekluk/projects/Frontiers-FloraRecognition/experiments/focalloss/wandb/run-20220519_115730-dt8wmof0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/picekl/frontiers-plant-recognition/runs/dt8wmof0\" target=\"_blank\">vit_base_patch32_224-SGD-plateau-light-FocalLoss</a></strong> to <a href=\"https://wandb.ai/picekl/frontiers-plant-recognition\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fgvc.utils.wandb import init_wandb\n",
    "\n",
    "init_wandb(config, RUN_NAME, entity='picekl', project='frontiers-plant-recognition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Optimizers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['optimizer'] == 'AdamW':\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "elif config['optimizer'] == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=0.9)\n",
    "\n",
    "if config['scheduler'] =='plateau':\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=1, verbose=True, eps=1e-6)\n",
    "elif config['scheduler'] == 'cyclic_cosine':\n",
    "    CYCLES = 5\n",
    "    t_initial = config['epochs'] / CYCLES\n",
    "    scheduler = CosineLRScheduler(optimizer, t_initial=20, lr_min=0.0001, cycle_decay = 0.9, cycle_limit = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(torch.nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=2.5, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight,reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "\n",
    "        ce_loss = F.cross_entropy(logits, target, reduction=self.reduction, weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['loss'] == 'CrossEntropyLoss':\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "elif config['loss'] == 'FocalLoss':\n",
    "    criterion = FocalLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 11:57:34,724 INFO [Train model] start\n",
      "8964it [08:21, 17.89it/s]\n",
      "2022-05-19 12:06:47,289 DEBUG   Epoch 1 - avg_train_loss: 4.1097  avg_val_loss: 2.8771 F1: 15.79  Acc: 44.16 Recall@3: 57.57 time: 551s\n",
      "2022-05-19 12:06:47,291 DEBUG   Epoch 1 - Save Best Accuracy: 0.441593 Model\n",
      "2022-05-19 12:06:47,695 DEBUG   Epoch 1 - Save Best Loss: 2.8771 Model\n",
      "8964it [08:17, 18.01it/s]\n",
      "2022-05-19 12:15:56,744 DEBUG   Epoch 2 - avg_train_loss: 1.7059  avg_val_loss: 2.0122 F1: 28.15  Acc: 52.20 Recall@3: 66.57 time: 549s\n",
      "2022-05-19 12:15:56,746 DEBUG   Epoch 2 - Save Best Accuracy: 0.522031 Model\n",
      "2022-05-19 12:15:57,625 DEBUG   Epoch 2 - Save Best Loss: 2.0122 Model\n",
      "8964it [08:22, 17.86it/s]\n",
      "2022-05-19 12:25:11,687 DEBUG   Epoch 3 - avg_train_loss: 0.6062  avg_val_loss: 1.5288 F1: 37.64  Acc: 58.40 Recall@3: 72.54 time: 553s\n",
      "2022-05-19 12:25:11,689 DEBUG   Epoch 3 - Save Best Accuracy: 0.583954 Model\n",
      "2022-05-19 12:25:12,607 DEBUG   Epoch 3 - Save Best Loss: 1.5288 Model\n",
      "8964it [08:20, 17.90it/s]\n",
      "2022-05-19 12:34:25,428 DEBUG   Epoch 4 - avg_train_loss: 0.1281  avg_val_loss: 1.1870 F1: 44.04  Acc: 63.79 Recall@3: 76.77 time: 552s\n",
      "2022-05-19 12:34:25,430 DEBUG   Epoch 4 - Save Best Accuracy: 0.637896 Model\n",
      "2022-05-19 12:34:26,343 DEBUG   Epoch 4 - Save Best Loss: 1.1870 Model\n",
      "8964it [08:17, 18.02it/s]\n",
      "2022-05-19 12:43:35,714 DEBUG   Epoch 5 - avg_train_loss: 0.0339  avg_val_loss: 1.0771 F1: 46.86  Acc: 65.91 Recall@3: 78.23 time: 548s\n",
      "2022-05-19 12:43:35,716 DEBUG   Epoch 5 - Save Best Accuracy: 0.659140 Model\n",
      "2022-05-19 12:43:36,631 DEBUG   Epoch 5 - Save Best Loss: 1.0771 Model\n",
      "8964it [08:18, 17.99it/s]\n",
      "2022-05-19 12:52:47,085 DEBUG   Epoch 6 - avg_train_loss: 0.0155  avg_val_loss: 1.0371 F1: 47.27  Acc: 66.39 Recall@3: 78.79 time: 550s\n",
      "2022-05-19 12:52:47,087 DEBUG   Epoch 6 - Save Best Accuracy: 0.663858 Model\n",
      "2022-05-19 12:52:47,983 DEBUG   Epoch 6 - Save Best Loss: 1.0371 Model\n",
      "8964it [08:17, 18.02it/s]\n",
      "2022-05-19 13:01:57,541 DEBUG   Epoch 7 - avg_train_loss: 0.0095  avg_val_loss: 1.0216 F1: 47.64  Acc: 66.66 Recall@3: 79.01 time: 549s\n",
      "2022-05-19 13:01:57,543 DEBUG   Epoch 7 - Save Best Accuracy: 0.666647 Model\n",
      "2022-05-19 13:01:58,408 DEBUG   Epoch 7 - Save Best Loss: 1.0216 Model\n",
      "8964it [08:18, 17.98it/s]\n",
      "2022-05-19 13:11:09,166 DEBUG   Epoch 8 - avg_train_loss: 0.0067  avg_val_loss: 1.0102 F1: 48.09  Acc: 66.96 Recall@3: 79.15 time: 550s\n",
      "2022-05-19 13:11:09,168 DEBUG   Epoch 8 - Save Best Accuracy: 0.669555 Model\n",
      "2022-05-19 13:11:10,029 DEBUG   Epoch 8 - Save Best Loss: 1.0102 Model\n",
      "8964it [08:19, 17.96it/s]\n",
      "2022-05-19 13:20:21,250 DEBUG   Epoch 9 - avg_train_loss: 0.0051  avg_val_loss: 1.0016 F1: 48.20  Acc: 67.07 Recall@3: 79.14 time: 550s\n",
      "2022-05-19 13:20:21,252 DEBUG   Epoch 9 - Save Best Accuracy: 0.670652 Model\n",
      "2022-05-19 13:20:22,172 DEBUG   Epoch 9 - Save Best Loss: 1.0016 Model\n",
      "8964it [08:18, 17.97it/s]\n",
      "2022-05-19 13:29:33,290 DEBUG   Epoch 10 - avg_train_loss: 0.0041  avg_val_loss: 0.9932 F1: 48.36  Acc: 67.20 Recall@3: 79.33 time: 550s\n",
      "2022-05-19 13:29:33,292 DEBUG   Epoch 10 - Save Best Accuracy: 0.672017 Model\n",
      "2022-05-19 13:29:34,229 DEBUG   Epoch 10 - Save Best Loss: 0.9932 Model\n",
      "8964it [08:17, 18.03it/s]\n",
      "2022-05-19 13:38:43,469 DEBUG   Epoch 11 - avg_train_loss: 0.0033  avg_val_loss: 0.9946 F1: 48.36  Acc: 67.18 Recall@3: 79.21 time: 548s\n",
      "8964it [08:19, 17.96it/s]\n",
      "2022-05-19 13:47:53,573 DEBUG   Epoch 12 - avg_train_loss: 0.0028  avg_val_loss: 0.9921 F1: 48.60  Acc: 67.25 Recall@3: 79.35 time: 550s\n",
      "2022-05-19 13:47:53,575 DEBUG   Epoch 12 - Save Best Accuracy: 0.672492 Model\n",
      "2022-05-19 13:47:54,492 DEBUG   Epoch 12 - Save Best Loss: 0.9921 Model\n",
      "8964it [08:19, 17.95it/s]\n",
      "2022-05-19 13:57:06,006 DEBUG   Epoch 13 - avg_train_loss: 0.0024  avg_val_loss: 0.9894 F1: 48.73  Acc: 67.29 Recall@3: 79.38 time: 551s\n",
      "2022-05-19 13:57:06,008 DEBUG   Epoch 13 - Save Best Accuracy: 0.672937 Model\n",
      "2022-05-19 13:57:06,929 DEBUG   Epoch 13 - Save Best Loss: 0.9894 Model\n",
      "8964it [08:16, 18.05it/s]\n",
      "2022-05-19 14:06:15,554 DEBUG   Epoch 14 - avg_train_loss: 0.0021  avg_val_loss: 0.9856 F1: 48.85  Acc: 67.39 Recall@3: 79.40 time: 548s\n",
      "2022-05-19 14:06:15,556 DEBUG   Epoch 14 - Save Best Accuracy: 0.673887 Model\n",
      "2022-05-19 14:06:16,433 DEBUG   Epoch 14 - Save Best Loss: 0.9856 Model\n",
      "8964it [08:19, 17.96it/s]\n",
      "2022-05-19 14:15:27,538 DEBUG   Epoch 15 - avg_train_loss: 0.0019  avg_val_loss: 0.9846 F1: 48.90  Acc: 67.39 Recall@3: 79.44 time: 550s\n",
      "2022-05-19 14:15:27,540 DEBUG   Epoch 15 - Save Best Loss: 0.9846 Model\n",
      "8964it [08:17, 18.01it/s]\n",
      "2022-05-19 14:24:37,431 DEBUG   Epoch 16 - avg_train_loss: 0.0017  avg_val_loss: 0.9818 F1: 48.87  Acc: 67.45 Recall@3: 79.55 time: 549s\n",
      "2022-05-19 14:24:37,433 DEBUG   Epoch 16 - Save Best Accuracy: 0.674450 Model\n",
      "2022-05-19 14:24:38,358 DEBUG   Epoch 16 - Save Best Loss: 0.9818 Model\n",
      "8964it [08:15, 18.08it/s]\n",
      "2022-05-19 14:33:46,322 DEBUG   Epoch 17 - avg_train_loss: 0.0016  avg_val_loss: 0.9809 F1: 49.10  Acc: 67.53 Recall@3: 79.47 time: 547s\n",
      "2022-05-19 14:33:46,324 DEBUG   Epoch 17 - Save Best Accuracy: 0.675281 Model\n",
      "2022-05-19 14:33:47,263 DEBUG   Epoch 17 - Save Best Loss: 0.9809 Model\n",
      "8964it [08:16, 18.06it/s]\n",
      "2022-05-19 14:42:55,943 DEBUG   Epoch 18 - avg_train_loss: 0.0014  avg_val_loss: 0.9802 F1: 48.94  Acc: 67.48 Recall@3: 79.50 time: 548s\n",
      "2022-05-19 14:42:55,945 DEBUG   Epoch 18 - Save Best Loss: 0.9802 Model\n",
      "8964it [08:18, 17.96it/s]\n",
      "2022-05-19 14:52:07,195 DEBUG   Epoch 19 - avg_train_loss: 0.0013  avg_val_loss: 0.9793 F1: 49.04  Acc: 67.48 Recall@3: 79.61 time: 550s\n",
      "2022-05-19 14:52:07,197 DEBUG   Epoch 19 - Save Best Loss: 0.9793 Model\n",
      "8964it [08:19, 17.94it/s]\n",
      "2022-05-19 15:01:19,326 DEBUG   Epoch 20 - avg_train_loss: 0.0012  avg_val_loss: 0.9783 F1: 49.26  Acc: 67.55 Recall@3: 79.60 time: 551s\n",
      "2022-05-19 15:01:19,328 DEBUG   Epoch 20 - Save Best Accuracy: 0.675518 Model\n",
      "2022-05-19 15:01:20,245 DEBUG   Epoch 20 - Save Best Loss: 0.9783 Model\n",
      "8964it [08:19, 17.96it/s]\n",
      "2022-05-19 15:10:31,272 DEBUG   Epoch 21 - avg_train_loss: 0.0011  avg_val_loss: 0.9752 F1: 49.27  Acc: 67.60 Recall@3: 79.61 time: 550s\n",
      "2022-05-19 15:10:31,274 DEBUG   Epoch 21 - Save Best Accuracy: 0.675993 Model\n",
      "2022-05-19 15:10:31,961 DEBUG   Epoch 21 - Save Best Loss: 0.9752 Model\n",
      "8964it [08:17, 18.02it/s]\n",
      "2022-05-19 15:19:41,537 DEBUG   Epoch 22 - avg_train_loss: 0.0010  avg_val_loss: 0.9761 F1: 49.34  Acc: 67.59 Recall@3: 79.60 time: 549s\n",
      "8964it [08:19, 17.96it/s]\n",
      "2022-05-19 15:28:52,104 DEBUG   Epoch 23 - avg_train_loss: 0.0010  avg_val_loss: 0.9746 F1: 49.25  Acc: 67.54 Recall@3: 79.65 time: 551s\n",
      "2022-05-19 15:28:52,107 DEBUG   Epoch 23 - Save Best Loss: 0.9746 Model\n",
      "8964it [08:20, 17.91it/s]\n",
      "2022-05-19 15:38:04,961 DEBUG   Epoch 24 - avg_train_loss: 0.0009  avg_val_loss: 0.9745 F1: 49.35  Acc: 67.64 Recall@3: 79.58 time: 552s\n",
      "2022-05-19 15:38:04,963 DEBUG   Epoch 24 - Save Best Accuracy: 0.676438 Model\n",
      "2022-05-19 15:38:05,901 DEBUG   Epoch 24 - Save Best Loss: 0.9745 Model\n",
      "8964it [08:16, 18.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    25: reducing learning rate of group 0 to 9.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 15:47:15,150 DEBUG   Epoch 25 - avg_train_loss: 0.0009  avg_val_loss: 0.9747 F1: 49.34  Acc: 67.67 Recall@3: 79.63 time: 548s\n",
      "2022-05-19 15:47:15,152 DEBUG   Epoch 25 - Save Best Accuracy: 0.676735 Model\n",
      "8964it [08:20, 17.92it/s]\n",
      "2022-05-19 15:56:27,827 DEBUG   Epoch 26 - avg_train_loss: 0.0008  avg_val_loss: 0.9740 F1: 49.31  Acc: 67.65 Recall@3: 79.65 time: 552s\n",
      "2022-05-19 15:56:27,829 DEBUG   Epoch 26 - Save Best Loss: 0.9740 Model\n",
      "8964it [08:15, 18.08it/s]\n",
      "2022-05-19 16:05:36,056 DEBUG   Epoch 27 - avg_train_loss: 0.0008  avg_val_loss: 0.9729 F1: 49.35  Acc: 67.67 Recall@3: 79.74 time: 547s\n",
      "2022-05-19 16:05:36,058 DEBUG   Epoch 27 - Save Best Loss: 0.9729 Model\n",
      "8964it [08:22, 17.82it/s]\n",
      "2022-05-19 16:14:51,402 DEBUG   Epoch 28 - avg_train_loss: 0.0008  avg_val_loss: 0.9734 F1: 49.25  Acc: 67.65 Recall@3: 79.61 time: 554s\n",
      "8964it [08:16, 18.06it/s]\n",
      "2022-05-19 16:23:59,103 DEBUG   Epoch 29 - avg_train_loss: 0.0007  avg_val_loss: 0.9727 F1: 49.21  Acc: 67.64 Recall@3: 79.69 time: 548s\n",
      "2022-05-19 16:23:59,105 DEBUG   Epoch 29 - Save Best Loss: 0.9727 Model\n",
      "8964it [08:16, 18.04it/s]\n",
      "2022-05-19 16:33:08,428 DEBUG   Epoch 30 - avg_train_loss: 0.0007  avg_val_loss: 0.9736 F1: 49.44  Acc: 67.67 Recall@3: 79.72 time: 548s\n",
      "8964it [08:19, 17.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    31: reducing learning rate of group 0 to 8.1000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 16:42:19,386 DEBUG   Epoch 31 - avg_train_loss: 0.0007  avg_val_loss: 0.9728 F1: 49.40  Acc: 67.68 Recall@3: 79.71 time: 551s\n",
      "2022-05-19 16:42:19,387 DEBUG   Epoch 31 - Save Best Accuracy: 0.676765 Model\n",
      "8964it [08:20, 17.91it/s]\n",
      "2022-05-19 16:51:32,408 DEBUG   Epoch 32 - avg_train_loss: 0.0006  avg_val_loss: 0.9705 F1: 49.46  Acc: 67.74 Recall@3: 79.78 time: 552s\n",
      "2022-05-19 16:51:32,410 DEBUG   Epoch 32 - Save Best Accuracy: 0.677358 Model\n",
      "2022-05-19 16:51:33,251 DEBUG   Epoch 32 - Save Best Loss: 0.9705 Model\n",
      "8964it [08:21, 17.87it/s]\n",
      "2022-05-19 17:00:47,261 DEBUG   Epoch 33 - avg_train_loss: 0.0006  avg_val_loss: 0.9710 F1: 49.46  Acc: 67.70 Recall@3: 79.76 time: 553s\n",
      "8964it [08:20, 17.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    34: reducing learning rate of group 0 to 7.2900e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 17:09:59,616 DEBUG   Epoch 34 - avg_train_loss: 0.0006  avg_val_loss: 0.9705 F1: 49.49  Acc: 67.79 Recall@3: 79.73 time: 552s\n",
      "2022-05-19 17:09:59,617 DEBUG   Epoch 34 - Save Best Accuracy: 0.677922 Model\n",
      "8964it [08:22, 17.85it/s]\n",
      "2022-05-19 17:19:14,189 DEBUG   Epoch 35 - avg_train_loss: 0.0006  avg_val_loss: 0.9695 F1: 49.44  Acc: 67.80 Recall@3: 79.77 time: 554s\n",
      "2022-05-19 17:19:14,191 DEBUG   Epoch 35 - Save Best Accuracy: 0.678011 Model\n",
      "2022-05-19 17:19:15,106 DEBUG   Epoch 35 - Save Best Loss: 0.9695 Model\n",
      "8964it [08:19, 17.95it/s]\n",
      "2022-05-19 17:28:26,868 DEBUG   Epoch 36 - avg_train_loss: 0.0006  avg_val_loss: 0.9706 F1: 49.56  Acc: 67.81 Recall@3: 79.75 time: 551s\n",
      "2022-05-19 17:28:26,870 DEBUG   Epoch 36 - Save Best Accuracy: 0.678070 Model\n",
      "8964it [08:17, 18.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    37: reducing learning rate of group 0 to 6.5610e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 17:37:36,841 DEBUG   Epoch 37 - avg_train_loss: 0.0006  avg_val_loss: 0.9699 F1: 49.58  Acc: 67.80 Recall@3: 79.77 time: 549s\n",
      "8964it [08:19, 17.94it/s]\n",
      "2022-05-19 17:46:48,134 DEBUG   Epoch 38 - avg_train_loss: 0.0005  avg_val_loss: 0.9674 F1: 49.53  Acc: 67.84 Recall@3: 79.80 time: 551s\n",
      "2022-05-19 17:46:48,136 DEBUG   Epoch 38 - Save Best Accuracy: 0.678397 Model\n",
      "2022-05-19 17:46:48,974 DEBUG   Epoch 38 - Save Best Loss: 0.9674 Model\n",
      "8964it [08:22, 17.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    48: reducing learning rate of group 0 to 4.7830e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 19:18:44,455 DEBUG   Epoch 48 - avg_train_loss: 0.0004  avg_val_loss: 0.9672 F1: 49.74  Acc: 67.88 Recall@3: 79.79 time: 554s\n",
      "2022-05-19 19:18:44,457 DEBUG   Epoch 48 - Save Best Accuracy: 0.678753 Model\n",
      "8964it [08:19, 17.95it/s]\n",
      "2022-05-19 19:27:56,436 DEBUG   Epoch 49 - avg_train_loss: 0.0004  avg_val_loss: 0.9670 F1: 49.59  Acc: 67.86 Recall@3: 79.85 time: 551s\n",
      "8964it [08:21, 17.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    50: reducing learning rate of group 0 to 4.3047e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 19:37:09,298 DEBUG   Epoch 50 - avg_train_loss: 0.0004  avg_val_loss: 0.9671 F1: 49.64  Acc: 67.87 Recall@3: 79.85 time: 553s\n",
      "8964it [08:17, 18.02it/s]\n",
      "2022-05-19 19:46:18,453 DEBUG   Epoch 51 - avg_train_loss: 0.0004  avg_val_loss: 0.9668 F1: 49.67  Acc: 67.87 Recall@3: 79.84 time: 549s\n",
      "8964it [08:18, 17.97it/s]\n",
      "2022-05-19 19:55:28,837 DEBUG   Epoch 52 - avg_train_loss: 0.0004  avg_val_loss: 0.9662 F1: 49.70  Acc: 67.83 Recall@3: 79.87 time: 550s\n",
      "2022-05-19 19:55:28,839 DEBUG   Epoch 52 - Save Best Loss: 0.9662 Model\n",
      "8964it [08:18, 17.98it/s]\n",
      "2022-05-19 20:04:40,155 DEBUG   Epoch 53 - avg_train_loss: 0.0004  avg_val_loss: 0.9667 F1: 49.74  Acc: 67.88 Recall@3: 79.85 time: 550s\n",
      "2022-05-19 20:04:40,157 DEBUG   Epoch 53 - Save Best Accuracy: 0.678812 Model\n",
      "8964it [08:18, 17.97it/s]\n",
      "2022-05-19 20:13:51,483 DEBUG   Epoch 54 - avg_train_loss: 0.0004  avg_val_loss: 0.9655 F1: 49.80  Acc: 67.91 Recall@3: 79.87 time: 550s\n",
      "2022-05-19 20:13:51,485 DEBUG   Epoch 54 - Save Best Accuracy: 0.679109 Model\n",
      "2022-05-19 20:13:52,279 DEBUG   Epoch 54 - Save Best Loss: 0.9655 Model\n",
      "8964it [08:19, 17.96it/s]\n",
      "2022-05-19 20:23:03,817 DEBUG   Epoch 55 - avg_train_loss: 0.0004  avg_val_loss: 0.9657 F1: 49.74  Acc: 67.91 Recall@3: 79.84 time: 551s\n",
      "2022-05-19 20:23:03,819 DEBUG   Epoch 55 - Save Best Accuracy: 0.679138 Model\n",
      "8964it [08:16, 18.04it/s]\n",
      "2022-05-19 20:32:12,720 DEBUG   Epoch 56 - avg_train_loss: 0.0004  avg_val_loss: 0.9650 F1: 49.77  Acc: 67.86 Recall@3: 79.86 time: 548s\n",
      "2022-05-19 20:32:12,721 DEBUG   Epoch 56 - Save Best Loss: 0.9650 Model\n",
      "8964it [08:21, 17.88it/s]\n",
      "2022-05-19 20:41:26,588 DEBUG   Epoch 57 - avg_train_loss: 0.0004  avg_val_loss: 0.9653 F1: 49.81  Acc: 67.90 Recall@3: 79.84 time: 553s\n",
      "8964it [08:19, 17.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    58: reducing learning rate of group 0 to 3.8742e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 20:50:37,495 DEBUG   Epoch 58 - avg_train_loss: 0.0004  avg_val_loss: 0.9655 F1: 49.74  Acc: 67.85 Recall@3: 79.81 time: 551s\n",
      "8964it [08:21, 17.88it/s]\n",
      "2022-05-19 20:59:50,275 DEBUG   Epoch 59 - avg_train_loss: 0.0004  avg_val_loss: 0.9661 F1: 49.74  Acc: 67.87 Recall@3: 79.83 time: 553s\n",
      "8964it [08:18, 17.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    60: reducing learning rate of group 0 to 3.4868e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 21:09:00,448 DEBUG   Epoch 60 - avg_train_loss: 0.0004  avg_val_loss: 0.9655 F1: 49.77  Acc: 67.90 Recall@3: 79.83 time: 550s\n",
      "8964it [08:17, 18.03it/s]\n",
      "2022-05-19 21:18:09,705 DEBUG   Epoch 61 - avg_train_loss: 0.0004  avg_val_loss: 0.9652 F1: 49.66  Acc: 67.89 Recall@3: 79.83 time: 549s\n",
      "8964it [08:14, 18.11it/s]\n",
      "2022-05-19 21:27:16,397 DEBUG   Epoch 62 - avg_train_loss: 0.0004  avg_val_loss: 0.9648 F1: 49.67  Acc: 67.88 Recall@3: 79.82 time: 547s\n",
      "2022-05-19 21:27:16,399 DEBUG   Epoch 62 - Save Best Loss: 0.9648 Model\n",
      "8964it [08:16, 18.04it/s]\n",
      "2022-05-19 21:36:26,423 DEBUG   Epoch 63 - avg_train_loss: 0.0004  avg_val_loss: 0.9643 F1: 49.83  Acc: 67.94 Recall@3: 79.89 time: 549s\n",
      "2022-05-19 21:36:26,425 DEBUG   Epoch 63 - Save Best Accuracy: 0.679405 Model\n",
      "2022-05-19 21:36:27,334 DEBUG   Epoch 63 - Save Best Loss: 0.9643 Model\n",
      "8964it [08:15, 18.09it/s]\n",
      "2022-05-19 21:45:35,527 DEBUG   Epoch 64 - avg_train_loss: 0.0004  avg_val_loss: 0.9648 F1: 49.79  Acc: 67.92 Recall@3: 79.85 time: 547s\n",
      "8964it [08:14, 18.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    65: reducing learning rate of group 0 to 3.1381e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 21:54:41,763 DEBUG   Epoch 65 - avg_train_loss: 0.0004  avg_val_loss: 0.9651 F1: 49.77  Acc: 67.91 Recall@3: 79.89 time: 546s\n",
      "8964it [08:15, 18.11it/s]\n",
      "2022-05-19 22:03:49,008 DEBUG   Epoch 66 - avg_train_loss: 0.0004  avg_val_loss: 0.9660 F1: 49.67  Acc: 67.84 Recall@3: 79.86 time: 547s\n",
      "8964it [08:15, 18.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    67: reducing learning rate of group 0 to 2.8243e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 22:12:56,225 DEBUG   Epoch 67 - avg_train_loss: 0.0004  avg_val_loss: 0.9652 F1: 49.77  Acc: 67.90 Recall@3: 79.84 time: 547s\n",
      "8964it [08:16, 18.06it/s]\n",
      "2022-05-19 22:22:04,554 DEBUG   Epoch 68 - avg_train_loss: 0.0004  avg_val_loss: 0.9657 F1: 49.75  Acc: 67.89 Recall@3: 79.87 time: 548s\n",
      "8964it [08:15, 18.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    69: reducing learning rate of group 0 to 2.5419e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 22:31:12,175 DEBUG   Epoch 69 - avg_train_loss: 0.0004  avg_val_loss: 0.9644 F1: 49.72  Acc: 67.87 Recall@3: 79.85 time: 548s\n",
      "8964it [08:13, 18.16it/s]\n",
      "2022-05-19 22:40:17,676 DEBUG   Epoch 70 - avg_train_loss: 0.0003  avg_val_loss: 0.9646 F1: 49.82  Acc: 67.93 Recall@3: 79.80 time: 546s\n",
      "8964it [08:13, 18.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    71: reducing learning rate of group 0 to 2.2877e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 22:49:22,896 DEBUG   Epoch 71 - avg_train_loss: 0.0004  avg_val_loss: 0.9644 F1: 49.77  Acc: 67.88 Recall@3: 79.82 time: 545s\n",
      "8964it [08:17, 18.02it/s]\n",
      "2022-05-19 22:58:32,361 DEBUG   Epoch 72 - avg_train_loss: 0.0003  avg_val_loss: 0.9640 F1: 49.74  Acc: 67.86 Recall@3: 79.85 time: 549s\n",
      "2022-05-19 22:58:32,364 DEBUG   Epoch 72 - Save Best Loss: 0.9640 Model\n",
      "8964it [08:14, 18.15it/s]\n",
      "2022-05-19 23:07:39,265 DEBUG   Epoch 73 - avg_train_loss: 0.0003  avg_val_loss: 0.9642 F1: 49.76  Acc: 67.89 Recall@3: 79.86 time: 546s\n",
      "8964it [08:16, 18.04it/s]\n",
      "2022-05-19 23:16:47,967 DEBUG   Epoch 74 - avg_train_loss: 0.0003  avg_val_loss: 0.9639 F1: 49.75  Acc: 67.86 Recall@3: 79.87 time: 549s\n",
      "2022-05-19 23:16:47,969 DEBUG   Epoch 74 - Save Best Loss: 0.9639 Model\n",
      "8964it [08:16, 18.04it/s]\n",
      "2022-05-19 23:25:57,754 DEBUG   Epoch 75 - avg_train_loss: 0.0003  avg_val_loss: 0.9644 F1: 49.77  Acc: 67.88 Recall@3: 79.84 time: 549s\n",
      "5014it [04:37, 18.21it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "8964it [08:17, 18.02it/s]\n",
      "2022-05-19 23:44:14,769 DEBUG   Epoch 77 - avg_train_loss: 0.0003  avg_val_loss: 0.9642 F1: 49.73  Acc: 67.89 Recall@3: 79.82 time: 549s\n",
      "8964it [08:15, 18.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    78: reducing learning rate of group 0 to 1.8530e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 23:53:22,151 DEBUG   Epoch 78 - avg_train_loss: 0.0003  avg_val_loss: 0.9643 F1: 49.82  Acc: 67.91 Recall@3: 79.86 time: 547s\n",
      "8964it [08:16, 18.04it/s]\n",
      "2022-05-20 00:02:30,755 DEBUG   Epoch 79 - avg_train_loss: 0.0003  avg_val_loss: 0.9639 F1: 49.80  Acc: 67.91 Recall@3: 79.83 time: 549s\n",
      "2022-05-20 00:02:30,757 DEBUG   Epoch 79 - Save Best Loss: 0.9639 Model\n",
      "8964it [08:16, 18.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    80: reducing learning rate of group 0 to 1.6677e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 00:11:39,831 DEBUG   Epoch 80 - avg_train_loss: 0.0003  avg_val_loss: 0.9644 F1: 49.75  Acc: 67.89 Recall@3: 79.85 time: 548s\n",
      "2787it [02:33, 17.49it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "8964it [08:15, 18.11it/s]\n",
      "2022-05-20 00:39:00,610 DEBUG   Epoch 83 - avg_train_loss: 0.0003  avg_val_loss: 0.9641 F1: 49.80  Acc: 67.89 Recall@3: 79.87 time: 547s\n",
      "1285it [01:10, 18.32it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "4969it [04:34, 17.49it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "8964it [08:15, 18.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    86: reducing learning rate of group 0 to 1.2158e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 01:06:21,100 DEBUG   Epoch 86 - avg_train_loss: 0.0003  avg_val_loss: 0.9645 F1: 49.78  Acc: 67.87 Recall@3: 79.86 time: 547s\n",
      "3245it [02:59, 17.96it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "6951it [06:25, 17.45it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "5090it [04:43, 18.59it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "8798it [08:09, 18.53it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "6987it [06:24, 18.02it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "8964it [08:13, 18.17it/s]\n",
      "2022-05-20 02:10:20,785 DEBUG   Epoch 93 - avg_train_loss: 0.0003  avg_val_loss: 0.9641 F1: 49.79  Acc: 67.89 Recall@3: 79.87 time: 545s\n",
      "1777it [01:38, 17.81it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "8964it [08:17, 18.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    96: reducing learning rate of group 0 to 7.9766e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 02:37:46,822 DEBUG   Epoch 96 - avg_train_loss: 0.0003  avg_val_loss: 0.9638 F1: 49.81  Acc: 67.89 Recall@3: 79.86 time: 549s\n",
      "2022-05-20 02:37:46,823 DEBUG   Epoch 96 - Save Best Loss: 0.9638 Model\n",
      "1477it [01:21, 17.90it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "8964it [08:15, 18.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    99: reducing learning rate of group 0 to 7.1790e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-20 03:05:12,224 DEBUG   Epoch 99 - avg_train_loss: 0.0003  avg_val_loss: 0.9637 F1: 49.76  Acc: 67.87 Recall@3: 79.92 time: 548s\n",
      "2687it [02:27, 18.31it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with timer('Train model', LOGGER):\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    \n",
    "    best_score = 0.\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        num_steps_per_epoch = len(train_loader)\n",
    "        num_updates = epoch * num_steps_per_epoch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_lbls = np.zeros((len(train_metadata)))\n",
    "        train_preds = np.zeros((len(train_metadata)))\n",
    "\n",
    "        for i, (images, labels, _) in tqdm.tqdm(enumerate(train_loader)):\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            y_preds = model(images)\n",
    "            loss = criterion(y_preds, labels)\n",
    "            \n",
    "            # Scale the loss to the mean of the accumulated batch size\n",
    "            avg_loss += loss.item() / len(train_loader) \n",
    "            loss = loss / config['accumulation_steps']\n",
    "            loss.backward()\n",
    "            if (i - 1) % config['accumulation_steps'] == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            if config['scheduler'] == 'cyclic_cosine':\n",
    "                num_updates += 1\n",
    "                scheduler.step_update(num_updates=num_updates)\n",
    "                \n",
    "                \n",
    "            train_preds[i * len(labels): (i+1) * len(labels)] = y_preds.argmax(1).to('cpu').numpy()\n",
    "            train_lbls[i * len(labels): (i+1) * len(labels)] = labels.to('cpu').numpy()\n",
    "            \n",
    "        model.eval()\n",
    "        avg_val_loss = 0.\n",
    "        preds = np.zeros((len(valid_dataset)))\n",
    "        preds_raw = []\n",
    "\n",
    "        for i, (images, labels, _) in enumerate(valid_loader):\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y_preds = model(images)\n",
    "            \n",
    "            preds[i * len(images): (i+1) * len(images)] = y_preds.argmax(1).to('cpu').numpy()\n",
    "            preds_raw.extend(y_preds.to('cpu').numpy())\n",
    "\n",
    "            loss = criterion(y_preds, labels)\n",
    "        \n",
    "            avg_val_loss += loss.item() / len(valid_loader)\n",
    "        \n",
    "        \n",
    "        if config['scheduler'] == 'plateau':\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif config['scheduler'] == 'cyclic_cosine':\n",
    "            scheduler.step(epoch + 1)\n",
    "        \n",
    "        train_accuracy = accuracy_score(train_lbls, train_preds)\n",
    "        train_f1 = f1_score(train_lbls, train_preds, average='macro')\n",
    "        \n",
    "        accuracy = accuracy_score(val_metadata['class_id'], preds)\n",
    "        f1 = f1_score(val_metadata['class_id'], preds, average='macro')\n",
    "        recall_3 = top_k_accuracy_score(val_metadata['class_id'], preds_raw, k=3)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        LOGGER.debug(f'  Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f} F1: {f1*100:.2f}  Acc: {accuracy*100:.2f} Recall@3: {recall_3*100:.2f} time: {elapsed:.0f}s')\n",
    "       \n",
    "        wandb.log({'Train_loss (avr.)': avg_loss,\n",
    "                   'Val. loss (avr.)': avg_val_loss,\n",
    "                   'Val. F1': np.round(f1*100, 2),\n",
    "                   'Val. Accuracy': np.round(accuracy*100, 2),\n",
    "                   'Val. Recall@3': np.round(recall_3*100, 2),\n",
    "                   'Learning Rate': optimizer.param_groups[0][\"lr\"],\n",
    "                   'Train. Accuracy': np.round(train_accuracy*100, 2),\n",
    "                   'Train. F1': np.round(train_f1*100, 2)})\n",
    "\n",
    "        if accuracy>best_score:\n",
    "            best_score = accuracy\n",
    "            LOGGER.debug(f'  Epoch {epoch+1} - Save Best Accuracy: {best_score:.6f} Model')\n",
    "            torch.save(model.state_dict(), f'{RUN_NAME}_best_accuracy.pth')\n",
    "\n",
    "        if avg_val_loss<best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            LOGGER.debug(f'  Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "            torch.save(model.state_dict(), f'{RUN_NAME}_best_loss.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'{RUN_NAME}-100E.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
