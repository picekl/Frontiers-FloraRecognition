{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Jupyter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "from sklearn.metrics import f1_score, accuracy_score, top_k_accuracy_score\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pd.set_option('display.max_columns', None)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fgvc.utils.datasets import TaxonomyDataset\n",
    "from fgvc.utils.augmentations import light_transforms\n",
    "from fgvc.utils.utils import timer, init_logger, seed_everything, getModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May  8 22:47:23 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.103.01   Driver Version: 470.103.01   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 38%   84C    P2   167W / 215W |   3525MiB /  7982MiB |     90%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 81%   60C    P8    16W / 340W |    232MiB / 10014MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce ...  Off  | 00000000:0A:00.0 Off |                  N/A |\n",
      "| 46%   51C    P8    23W / 250W |     10MiB /  7982MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286841\n",
      "33703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1013357/351286042.py:4: DtypeWarning: Columns (15,16,17,19,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  val_metadata = pd.read_csv(\"../../metadata/PlantCLEF2018_val_metadata.csv\")\n"
     ]
    }
   ],
   "source": [
    "train_metadata = pd.read_csv(\"../../metadata/PlantCLEF2018_train_metadata.csv\")\n",
    "print(len(train_metadata))\n",
    "\n",
    "val_metadata = pd.read_csv(\"../../metadata/PlantCLEF2018_val_metadata.csv\")\n",
    "print(len(val_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata['image_path'] = train_metadata['image_path'].apply(lambda x: x.replace('../../../nahouby/Datasets/PlantCLEF/', '/Data-10T/PlantCLEF/'))\n",
    "train_metadata['image_path'] = train_metadata['image_path'].apply(lambda x: x.replace('../../nahouby/Datasets/PlantCLEF/', '/Data-10T/PlantCLEF/'))\n",
    "\n",
    "val_metadata['image_path'] = val_metadata['image_path'].apply(lambda x: x.replace('../../../nahouby/Datasets/PlantCLEF/', '/Data-10T/PlantCLEF/'))\n",
    "val_metadata['image_path'] = val_metadata['image_path'].apply(lambda x: x.replace('../../nahouby/Datasets/PlantCLEF/', '/Data-10T/PlantCLEF/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust BATCH_SIZE and ACCUMULATION_STEPS to values that if multiplied results in 64 !!!!!1\n",
    "\n",
    "config = {\"augmentations\": 'light',\n",
    "           \"optimizer\": 'SGD',\n",
    "           \"scheduler\": 'plateau',\n",
    "           \"image_size\": (224, 224),\n",
    "           \"random_seed\": 777,\n",
    "           \"number_of_classes\": len(train_metadata['class_id'].unique()),\n",
    "           \"architecture\": 'tax_former',\n",
    "           \"batch_size\": 64,\n",
    "           \"accumulation_steps\": 16,\n",
    "           \"epochs\": 100,\n",
    "           \"learning_rate\": 0.1,\n",
    "           \"dataset\": 'PlantCLEF2018',\n",
    "           \"loss\": 'CrossEntropyLoss',\n",
    "           \"training_samples\": len(train_metadata),\n",
    "           \"valid_samples\": len(val_metadata),\n",
    "           \"workers\": 12,\n",
    "           }\n",
    "\n",
    "RUN_NAME = f\"{config['architecture']}-{config['optimizer']}-{config['scheduler']}-{config['augmentations']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix Seeds & Log Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FILE = f'{RUN_NAME}.log'\n",
    "LOGGER = init_logger(LOG_FILE)\n",
    "\n",
    "seed_everything(config['random_seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = len(train_metadata['class_id'].unique())\n",
    "N_GENERA = len(train_metadata['genus_id'].unique())\n",
    "N_FAMILY = len(train_metadata['family_id'].unique())\n",
    "\n",
    "MODEL_NAME = 'tax_former'\n",
    "NUM_CLASSES = [N_CLASSES, N_GENERA, N_FAMILY]\n",
    "\n",
    "\n",
    "import timm\n",
    "model = timm.create_model(MODEL_NAME,  num_classes=NUM_CLASSES, pretrained=False)\n",
    "\n",
    "model_mean = list(model.default_cfg['mean'])\n",
    "model_std = list(model.default_cfg['std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust BATCH_SIZE and ACCUMULATION_STEPS to values that if multiplied results in 64 !!!!!1\n",
    "\n",
    "if config['augmentations'] == 'light':\n",
    "    train_augmentations = light_transforms(data='train', image_size=config['image_size'], mean=model_mean, std=model_std)\n",
    "    val_augmentations = light_transforms(data='valid', image_size=config['image_size'], mean=model_mean, std=model_std)\n",
    "\n",
    "train_dataset = TaxonomyDataset(train_metadata, transform=train_augmentations)\n",
    "valid_dataset = TaxonomyDataset(val_metadata, transform=val_augmentations)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=config['workers'])\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=config['workers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpicekl\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/projects/Frontiers-FGVC/experiments/architectures/wandb/run-20220508_224725-23a0bnxf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/picekl/frontiers-plant-recognition/runs/23a0bnxf\" target=\"_blank\">tax_former-SGD-plateau-light</a></strong> to <a href=\"https://wandb.ai/picekl/frontiers-plant-recognition\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fgvc.utils.wandb import init_wandb\n",
    "\n",
    "init_wandb(config, RUN_NAME, entity='picekl', project='frontiers-plant-recognition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Optimizers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['optimizer'] == 'AdamW':\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "elif config['optimizer'] == 'SGD':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=0.9)\n",
    "\n",
    "if config['scheduler'] =='plateau':\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=1, verbose=True, eps=1e-6)\n",
    "elif config['scheduler'] == 'cyclic_cosine':\n",
    "    CYCLES = 5\n",
    "    t_initial = config['epochs'] / CYCLES\n",
    "    scheduler = CosineLRScheduler(optimizer, t_initial=20, lr_min=0.0001, cycle_decay = 0.9, cycle_limit = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 22:47:28,168 INFO [Train model] start\n",
      "4482it [07:26, 10.04it/s]\n",
      "2022-05-08 22:55:59,151 DEBUG   Epoch 1 - avg_train_loss: 19.5109  avg_val_loss: 18.8333 F1: 0.02  Acc: 1.23 Recall@3: 3.04 time: 509s\n",
      "2022-05-08 22:55:59,153 DEBUG   Epoch 1 - Save Best Accuracy: 0.012343 Model\n",
      "2022-05-08 22:55:59,256 DEBUG   Epoch 1 - Save Best Loss: 18.8333 Model\n",
      "4482it [07:28, 10.00it/s]\n",
      "2022-05-08 23:04:31,162 DEBUG   Epoch 2 - avg_train_loss: 17.9473  avg_val_loss: 17.6753 F1: 0.13  Acc: 2.86 Recall@3: 6.02 time: 512s\n",
      "2022-05-08 23:04:31,164 DEBUG   Epoch 2 - Save Best Accuracy: 0.028573 Model\n",
      "2022-05-08 23:04:31,240 DEBUG   Epoch 2 - Save Best Loss: 17.6753 Model\n",
      "4482it [07:24, 10.08it/s]\n",
      "2022-05-08 23:13:03,320 DEBUG   Epoch 3 - avg_train_loss: 17.0138  avg_val_loss: 16.9941 F1: 0.31  Acc: 4.12 Recall@3: 8.32 time: 512s\n",
      "2022-05-08 23:13:03,321 DEBUG   Epoch 3 - Save Best Accuracy: 0.041213 Model\n",
      "2022-05-08 23:13:03,401 DEBUG   Epoch 3 - Save Best Loss: 16.9941 Model\n",
      "4482it [07:23, 10.10it/s]\n",
      "2022-05-08 23:21:31,029 DEBUG   Epoch 4 - avg_train_loss: 16.3194  avg_val_loss: 16.3582 F1: 0.58  Acc: 5.43 Recall@3: 10.71 time: 508s\n",
      "2022-05-08 23:21:31,031 DEBUG   Epoch 4 - Save Best Accuracy: 0.054298 Model\n",
      "2022-05-08 23:21:31,100 DEBUG   Epoch 4 - Save Best Loss: 16.3582 Model\n",
      "631it [01:01,  9.76it/s]"
     ]
    }
   ],
   "source": [
    "with timer('Train model', LOGGER):\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_score = 0.\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        \n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        num_steps_per_epoch = len(train_loader)\n",
    "        num_updates = epoch * num_steps_per_epoch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_lbls = np.zeros((len(train_metadata)))\n",
    "        train_preds = np.zeros((len(train_metadata)))\n",
    "\n",
    "        for i, (images, labels) in tqdm.tqdm(enumerate(train_loader)):\n",
    "                        \n",
    "            images = images.to(device)\n",
    "            \n",
    "            species_labels = labels['species'].to(device)\n",
    "            genus_labels = labels['genus'].to(device)\n",
    "            family_labels = labels['family'].to(device)\n",
    "\n",
    "            species_preds, genus_preds, family_preds = model(images)\n",
    "\n",
    "            loss_species = criterion(species_preds, species_labels)            \n",
    "            loss_genus   = criterion(genus_preds, genus_labels) \n",
    "            loss_family  = criterion(family_preds, family_labels) \n",
    "            loss = loss_species + loss_genus + loss_family  \n",
    "            \n",
    "            # Scale the loss to the mean of the accumulated batch size\n",
    "            avg_loss += loss.item() / len(train_loader) \n",
    "            loss = loss / config['accumulation_steps']\n",
    "            loss.backward()\n",
    "            if (i - 1) % config['accumulation_steps'] == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            if config['scheduler'] == 'cyclic_cosine':\n",
    "                num_updates += 1\n",
    "                scheduler.step_update(num_updates=num_updates)\n",
    "                \n",
    "                \n",
    "            train_preds[i * len(species_labels): (i+1) * len(species_labels)] = species_preds.argmax(1).to('cpu').numpy()\n",
    "            train_lbls[i * len(species_labels): (i+1) * len(species_labels)] = species_labels.to('cpu').numpy()\n",
    "            \n",
    "        model.eval()\n",
    "        avg_val_loss = 0.\n",
    "        preds = np.zeros((len(valid_dataset)))\n",
    "        preds_raw = []\n",
    "\n",
    "        for i, (images, labels) in enumerate(valid_loader):\n",
    "            \n",
    "            images = images.to(device)\n",
    "            \n",
    "            species_labels = labels['species'].to(device)\n",
    "            genus_labels = labels['genus'].to(device)\n",
    "            family_labels = labels['family'].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                 species_preds, genus_preds, family_preds = model(images)\n",
    "            \n",
    "            preds[i * len(images): (i+1) * len(images)] = species_preds.argmax(1).to('cpu').numpy()\n",
    "            preds_raw.extend(species_preds.to('cpu').numpy())\n",
    "\n",
    "            loss_species = criterion(species_preds, species_labels)            \n",
    "            loss_genus   = criterion(genus_preds, genus_labels) \n",
    "            loss_family  = criterion(family_preds, family_labels) \n",
    "            loss = loss_species + loss_genus + loss_family  \n",
    "                    \n",
    "            avg_val_loss += loss.item() / len(valid_loader)\n",
    "        \n",
    "        \n",
    "        if config['scheduler'] == 'plateau':\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif config['scheduler'] == 'cyclic_cosine':\n",
    "            scheduler.step(epoch + 1)\n",
    "        \n",
    "        train_accuracy = accuracy_score(train_lbls, train_preds)\n",
    "        train_f1 = f1_score(train_lbls, train_preds, average='macro')\n",
    "        \n",
    "        accuracy = accuracy_score(val_metadata['class_id'], preds)\n",
    "        f1 = f1_score(val_metadata['class_id'], preds, average='macro')\n",
    "        recall_3 = top_k_accuracy_score(val_metadata['class_id'], preds_raw, k=3)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        LOGGER.debug(f'  Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f} F1: {f1*100:.2f}  Acc: {accuracy*100:.2f} Recall@3: {recall_3*100:.2f} time: {elapsed:.0f}s')\n",
    "       \n",
    "        wandb.log({'Train_loss (avr.)': avg_loss,\n",
    "                   'Val. loss (avr.)': avg_val_loss,\n",
    "                   'Val. F1': np.round(f1*100, 2),\n",
    "                   'Val. Accuracy': np.round(accuracy*100, 2),\n",
    "                   'Val. Recall@3': np.round(recall_3*100, 2),\n",
    "                   'Learning Rate': optimizer.param_groups[0][\"lr\"],\n",
    "                   'Train. Accuracy': np.round(train_accuracy*100, 2),\n",
    "                   'Train. F1': np.round(train_f1*100, 2)})\n",
    "\n",
    "        if accuracy>best_score:\n",
    "            best_score = accuracy\n",
    "            LOGGER.debug(f'  Epoch {epoch+1} - Save Best Accuracy: {best_score:.6f} Model')\n",
    "            torch.save(model.state_dict(), f'{RUN_NAME}_best_accuracy.pth')\n",
    "\n",
    "        if avg_val_loss<best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            LOGGER.debug(f'  Epoch {epoch+1} - Save Best Loss: {best_loss:.4f} Model')\n",
    "            torch.save(model.state_dict(), f'{RUN_NAME}_best_loss.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'{RUN_NAME}-100E.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
